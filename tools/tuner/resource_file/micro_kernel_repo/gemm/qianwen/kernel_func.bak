#pragma once
#include "xetla.hpp"

using namespace gpu::xetla;

enum class fused_type : uint8_t {
    none = 0,
    bias = 1,
    bias_gelu = 2,
    res_add = 3
};

template <typename dtype_a, typename dtype_b, typename dtype_c,
        typename dtype_bias, typename dtype_res, typename dtype_acc,
        uint32_t wg_m, uint32_t wg_n, uint32_t sg_m, uint32_t sg_n,
        uint32_t sg_k, mem_layout layout_a, mem_layout layout_b,
        uint32_t global_kslicing, uint32_t local_kslicing, fused_type fused_op>
struct gemm_test_func {
    using tile_shape = group::tile_shape_t<wg_n, wg_m, sg_n, sg_m>;
    static constexpr uint32_t periodic_sync_interval = 1;
    static constexpr uint32_t prefetch_distance = 3;
    using gemm_t = typename group::gemm_selector_t<dtype_a, dtype_b, layout_a,
            layout_b, mem_space::global, mem_space::global, 8, 8, dtype_acc,
            tile_shape, sg_k, mma_engine::xmx, gpu_arch::Xe, prefetch_distance,
            periodic_sync_interval>::gemm;

    using post_op_t0 = subgroup::none_op_t;
    using post_op_t1 = subgroup::bias_add_op_t<dtype_bias, gpu_arch::Xe>;
    using post_op_t2 = subgroup::chained_tile_op_t<
            subgroup::bias_add_op_t<dtype_bias, gpu_arch::Xe>,
            subgroup::gelu_fwd_op_t>;
    using post_op_t3 = subgroup::chained_tile_op_t<
            subgroup::elemwise_reduce_op_t<reduce_op::sum, dtype_res,
                    gpu_arch::Xe>,
            subgroup::elemwise_reduce_op_t<reduce_op::sum, dtype_res,
                    gpu_arch::Xe>>;

    using post_op_t = typename std::conditional<fused_op == fused_type::none,
            post_op_t0,
            typename std::conditional<fused_op == fused_type::bias, post_op_t1,
                    typename std::conditional<fused_op == fused_type::bias_gelu,
                            post_op_t2, post_op_t3>::type>::type>::type;

    using epilogue_policy_t = typename std::conditional<(global_kslicing > 1),
            group::epilogue_policy_default<gpu_arch::Xe>,
            group::epilogue_policy_tile_op<post_op_t, gpu_arch::Xe>>::type;

    using epilogue_t = group::epilogue_t<epilogue_policy_t, tile_shape,
            mem_desc_t<dtype_c, mem_layout::row_major, mem_space::global>>;
    using gemm_op_t = kernel::gemm_universal_t<
            kernel::dispatch_policy_kslicing<global_kslicing, local_kslicing,
                    gpu_arch::Xe>,
            gemm_t, epilogue_t>;
    static constexpr uint32_t barrier_count = gemm_op_t::get_barrier_count();
    static constexpr uint32_t slm_size = gemm_op_t::get_slm_size();
    static const char *func_name() { return "gemm_test_func"; }

    static bool can_implement(dtype_a *A, dtype_b *B, dtype_c *C,
            uint32_t mat_m, uint32_t mat_n, uint32_t mat_k, uint32_t lda,
            uint32_t ldb, uint32_t ldc, dtype_bias *bias_ptr,
            dtype_res *res_ptr0, dtype_res *res_ptr1, dtype_acc *acc_ptr,
            uint32_t *cnt_ptr) {
        if constexpr ((global_kslicing > 1) || (fused_op == fused_type::none)) {
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr, {});
            return gemm_op_t::can_implement(arg);
        } else if constexpr (fused_op == fused_type::bias_gelu) {
            typename post_op_t2::arguments_t epilogue_args(
                    {{bias_ptr}, {mat_n, 1, ldc}}, {});
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr, epilogue_args);
            return gemm_op_t::can_implement(arg);
        } else if constexpr (fused_op == fused_type::bias) {
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr,
                    {{{bias_ptr}, {mat_n, 1, ldc}}});
            return gemm_op_t::can_implement(arg);
        } else if constexpr (fused_op == fused_type::res_add) {
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr,
                    {{{{res_ptr0}, {mat_n, mat_m, ldc}},
                            {{res_ptr1}, {mat_n, mat_m, ldc}}}});
            return gemm_op_t::can_implement(arg);
        }

        return true;
    }
    static inline void run(sycl::nd_item<3> &item, dtype_a *A, dtype_b *B,
            dtype_c *C, uint32_t mat_m, uint32_t mat_n, uint32_t mat_k,
            uint32_t lda, uint32_t ldb, uint32_t ldc, dtype_bias *bias_ptr,
            dtype_res *res_ptr0, dtype_res *res_ptr1, dtype_acc *acc_ptr,
            uint32_t *cnt_ptr) {
        gemm_op_t gemm_op;

        if constexpr ((global_kslicing > 1) || (fused_op == fused_type::none)) {
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr, {});
            gemm_op(item, arg);
        } else if constexpr (fused_op == fused_type::bias_gelu) {
            typename post_op_t2::arguments_t epilogue_args(
                    {{bias_ptr}, {mat_n, 1, ldc}}, {});
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr, epilogue_args);
            gemm_op(item, arg);
        } else if constexpr (fused_op == fused_type::bias) {
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr,
                    {{{bias_ptr}, {mat_n, 1, ldc}}});

            gemm_op(item, arg);
        } else if constexpr (fused_op == fused_type::res_add) {
            typename gemm_op_t::arguments_t arg(mat_m, mat_k, mat_n, A, lda, B,
                    ldb, C, ldc, acc_ptr, cnt_ptr,
                    {{{{res_ptr0}, {mat_n, mat_m, ldc}},
                            {{res_ptr1}, {mat_n, mat_m, ldc}}}});

            gemm_op(item, arg);
        }
    }
};